# <font color=blue>"对于梯度下降算法的初步认识" </font>

## <center> 金圣勋</center>

- [参考视频]:【【梯度下降】3D可视化讲解通俗易懂-哔哩哔哩】 <https://b23.tv/cIgQGj5.html>

### 1. 问题的引入

- 如下一个线性的拟合场景
  ![alt text][def]

[def]: 微信图片_20241019232729.jpg  

### 2. 预测函数

- 我们可以用y=mx 这样一个预测函数对数据点进行拟合,但难以量化拟合效果最好的m，所以要构建关于m的代价函数
  ![alt text][def2]
  
[def2]: 微信图片_20241019235448.jpg

### 3. 代价函数

- 代价函数的构建过程
  利用均方差，实现样本点拟合过程的转换
![alt text][def3]

[def3]: 微信图片_20241020000401.jpg

![alt text](微信图片_20241020000644.jpg)
=  梯度下降法
![alt text](微信图片_20241020004204.jpg)  

- 广义上来说 ，图中斜率应为梯度
- 这种方式保证了w 向着梯度减小的方向更新,而且能够在w越过梯度最小点时得以“矫正”回来
- 学习率的大小选择也是关键，太大来回震荡无法满足要求，太小无法满足要求，所以学习率一般是随迭代次数而衰减的
- 对于考核的思考
  大概||grad(Zactual,θ)||随着不断迭代而下降
  f(n)=|nZ0 -Σn Zactual|是减函数，当 lim n->∞时,limf(n)=0

### 4.梯度下降算法的必要性

- 对开始的例子稍加改动
  拟合直线为y=wx+b,代价函数会变成关于w和b的二次曲面
  ![alt text](微信图片_20241020012609.jpg)
- 或者 y不是线性或一元函,代价函数都会更复杂,一般的求解难以求平均误差最小值
  
### 5.三种常用的梯度算法

- BGD(每次用所有的数据,效果好但速度慢)
  ![alt text](微信图片_20241020013534.jpg)
- SGD(每次只用一组数据,效果好但速度快)
  ![alt text](微信图片_20241020013534-1.jpg)
- MGD(每次用一部分数据,兼具前两者优点)
  ![alt text](微信图片_20241020014153.jpg)
